{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_Mail.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_BmlBCQE50p"
      },
      "source": [
        "import os\r\n",
        "import io\r\n",
        "from random import seed\r\n",
        "from random import randrange\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from pandas import DataFrame\r\n",
        "from collections import Counter\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiGhEXY4FcQF",
        "outputId": "e9f3c1d5-f9c3-4875-e8d5-ddb9e1fe11a7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYibf_PEHHKy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd47ee00-c775-4723-ccf4-c6fd78c3597b"
      },
      "source": [
        "path = '/content/drive/MyDrive/data/emails.csv'\r\n",
        "all_data = pd.read_csv(path)\r\n",
        "print(all_data.shape)\r\n",
        "# all_data.head(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5172, 3002)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwbQ6hKAKZm1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "2349969f-b072-470c-9381-3f7ae9afe4ed"
      },
      "source": [
        "all_data.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>to</th>\n",
              "      <th>ect</th>\n",
              "      <th>and</th>\n",
              "      <th>for</th>\n",
              "      <th>of</th>\n",
              "      <th>a</th>\n",
              "      <th>you</th>\n",
              "      <th>hou</th>\n",
              "      <th>in</th>\n",
              "      <th>on</th>\n",
              "      <th>is</th>\n",
              "      <th>this</th>\n",
              "      <th>enron</th>\n",
              "      <th>i</th>\n",
              "      <th>be</th>\n",
              "      <th>that</th>\n",
              "      <th>will</th>\n",
              "      <th>have</th>\n",
              "      <th>with</th>\n",
              "      <th>your</th>\n",
              "      <th>at</th>\n",
              "      <th>we</th>\n",
              "      <th>s</th>\n",
              "      <th>are</th>\n",
              "      <th>it</th>\n",
              "      <th>by</th>\n",
              "      <th>com</th>\n",
              "      <th>as</th>\n",
              "      <th>from</th>\n",
              "      <th>gas</th>\n",
              "      <th>or</th>\n",
              "      <th>not</th>\n",
              "      <th>me</th>\n",
              "      <th>deal</th>\n",
              "      <th>if</th>\n",
              "      <th>meter</th>\n",
              "      <th>hpl</th>\n",
              "      <th>please</th>\n",
              "      <th>re</th>\n",
              "      <th>...</th>\n",
              "      <th>bold</th>\n",
              "      <th>catch</th>\n",
              "      <th>performing</th>\n",
              "      <th>accepted</th>\n",
              "      <th>matters</th>\n",
              "      <th>batch</th>\n",
              "      <th>continuing</th>\n",
              "      <th>winning</th>\n",
              "      <th>symbol</th>\n",
              "      <th>offsystem</th>\n",
              "      <th>decisions</th>\n",
              "      <th>produced</th>\n",
              "      <th>ended</th>\n",
              "      <th>greatest</th>\n",
              "      <th>degree</th>\n",
              "      <th>solmonson</th>\n",
              "      <th>imbalances</th>\n",
              "      <th>fall</th>\n",
              "      <th>fear</th>\n",
              "      <th>hate</th>\n",
              "      <th>fight</th>\n",
              "      <th>reallocated</th>\n",
              "      <th>debt</th>\n",
              "      <th>reform</th>\n",
              "      <th>australia</th>\n",
              "      <th>plain</th>\n",
              "      <th>prompt</th>\n",
              "      <th>remains</th>\n",
              "      <th>ifhsc</th>\n",
              "      <th>enhancements</th>\n",
              "      <th>connevey</th>\n",
              "      <th>jay</th>\n",
              "      <th>valued</th>\n",
              "      <th>lay</th>\n",
              "      <th>infrastructure</th>\n",
              "      <th>military</th>\n",
              "      <th>allowing</th>\n",
              "      <th>ff</th>\n",
              "      <th>dry</th>\n",
              "      <th>Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.00000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "      <td>5172.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>6.640565</td>\n",
              "      <td>6.188128</td>\n",
              "      <td>5.143852</td>\n",
              "      <td>3.075599</td>\n",
              "      <td>3.124710</td>\n",
              "      <td>2.627030</td>\n",
              "      <td>55.517401</td>\n",
              "      <td>2.466551</td>\n",
              "      <td>2.024362</td>\n",
              "      <td>10.600155</td>\n",
              "      <td>10.935808</td>\n",
              "      <td>5.386118</td>\n",
              "      <td>1.388631</td>\n",
              "      <td>1.335267</td>\n",
              "      <td>45.857889</td>\n",
              "      <td>3.229312</td>\n",
              "      <td>0.924401</td>\n",
              "      <td>0.850928</td>\n",
              "      <td>0.804718</td>\n",
              "      <td>0.939675</td>\n",
              "      <td>0.814385</td>\n",
              "      <td>6.932328</td>\n",
              "      <td>1.978732</td>\n",
              "      <td>41.811872</td>\n",
              "      <td>1.408546</td>\n",
              "      <td>4.501160</td>\n",
              "      <td>0.657386</td>\n",
              "      <td>1.767208</td>\n",
              "      <td>4.807425</td>\n",
              "      <td>0.813998</td>\n",
              "      <td>0.617363</td>\n",
              "      <td>7.749033</td>\n",
              "      <td>0.838167</td>\n",
              "      <td>5.332367</td>\n",
              "      <td>0.734532</td>\n",
              "      <td>1.225251</td>\n",
              "      <td>0.538090</td>\n",
              "      <td>0.635151</td>\n",
              "      <td>0.627804</td>\n",
              "      <td>9.109049</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005994</td>\n",
              "      <td>0.005607</td>\n",
              "      <td>0.004254</td>\n",
              "      <td>0.004254</td>\n",
              "      <td>0.004640</td>\n",
              "      <td>0.008701</td>\n",
              "      <td>0.005994</td>\n",
              "      <td>0.006381</td>\n",
              "      <td>0.008894</td>\n",
              "      <td>0.00406</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>0.005414</td>\n",
              "      <td>0.070766</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>0.008701</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.017208</td>\n",
              "      <td>0.006381</td>\n",
              "      <td>0.015855</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.004640</td>\n",
              "      <td>0.006381</td>\n",
              "      <td>0.005027</td>\n",
              "      <td>0.006961</td>\n",
              "      <td>0.037510</td>\n",
              "      <td>0.008894</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>0.005220</td>\n",
              "      <td>0.005607</td>\n",
              "      <td>0.005027</td>\n",
              "      <td>0.012568</td>\n",
              "      <td>0.010634</td>\n",
              "      <td>0.098028</td>\n",
              "      <td>0.004254</td>\n",
              "      <td>0.006574</td>\n",
              "      <td>0.004060</td>\n",
              "      <td>0.914733</td>\n",
              "      <td>0.006961</td>\n",
              "      <td>0.290023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>11.745009</td>\n",
              "      <td>9.534576</td>\n",
              "      <td>14.101142</td>\n",
              "      <td>6.045970</td>\n",
              "      <td>4.680522</td>\n",
              "      <td>6.229845</td>\n",
              "      <td>87.574172</td>\n",
              "      <td>4.314444</td>\n",
              "      <td>6.967878</td>\n",
              "      <td>19.281892</td>\n",
              "      <td>17.999402</td>\n",
              "      <td>9.144407</td>\n",
              "      <td>2.849708</td>\n",
              "      <td>4.570525</td>\n",
              "      <td>77.832221</td>\n",
              "      <td>5.045836</td>\n",
              "      <td>2.084255</td>\n",
              "      <td>2.065738</td>\n",
              "      <td>1.444839</td>\n",
              "      <td>2.036278</td>\n",
              "      <td>2.001731</td>\n",
              "      <td>12.949766</td>\n",
              "      <td>3.719254</td>\n",
              "      <td>66.530032</td>\n",
              "      <td>2.583277</td>\n",
              "      <td>8.377237</td>\n",
              "      <td>1.247460</td>\n",
              "      <td>11.002626</td>\n",
              "      <td>7.568755</td>\n",
              "      <td>1.657193</td>\n",
              "      <td>1.844452</td>\n",
              "      <td>12.538043</td>\n",
              "      <td>1.771383</td>\n",
              "      <td>8.256332</td>\n",
              "      <td>1.930954</td>\n",
              "      <td>2.375526</td>\n",
              "      <td>1.945471</td>\n",
              "      <td>1.516908</td>\n",
              "      <td>1.130827</td>\n",
              "      <td>14.494729</td>\n",
              "      <td>...</td>\n",
              "      <td>0.118664</td>\n",
              "      <td>0.077224</td>\n",
              "      <td>0.073462</td>\n",
              "      <td>0.070781</td>\n",
              "      <td>0.067969</td>\n",
              "      <td>0.210261</td>\n",
              "      <td>0.077195</td>\n",
              "      <td>0.123437</td>\n",
              "      <td>0.110895</td>\n",
              "      <td>0.08675</td>\n",
              "      <td>0.066569</td>\n",
              "      <td>0.080906</td>\n",
              "      <td>0.367953</td>\n",
              "      <td>0.066569</td>\n",
              "      <td>0.132372</td>\n",
              "      <td>0.087733</td>\n",
              "      <td>0.100112</td>\n",
              "      <td>0.191417</td>\n",
              "      <td>0.097135</td>\n",
              "      <td>0.140928</td>\n",
              "      <td>0.133259</td>\n",
              "      <td>0.073439</td>\n",
              "      <td>0.097135</td>\n",
              "      <td>0.073413</td>\n",
              "      <td>0.162025</td>\n",
              "      <td>0.239546</td>\n",
              "      <td>0.099885</td>\n",
              "      <td>0.066569</td>\n",
              "      <td>0.084428</td>\n",
              "      <td>0.192108</td>\n",
              "      <td>0.105788</td>\n",
              "      <td>0.199682</td>\n",
              "      <td>0.116693</td>\n",
              "      <td>0.569532</td>\n",
              "      <td>0.096252</td>\n",
              "      <td>0.138908</td>\n",
              "      <td>0.072145</td>\n",
              "      <td>2.780203</td>\n",
              "      <td>0.098086</td>\n",
              "      <td>0.453817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>62.250000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>210.000000</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>344.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>1898.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>223.000000</td>\n",
              "      <td>302.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>972.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>267.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>882.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>726.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>108.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>233.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 3001 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               the           to  ...          dry   Prediction\n",
              "count  5172.000000  5172.000000  ...  5172.000000  5172.000000\n",
              "mean      6.640565     6.188128  ...     0.006961     0.290023\n",
              "std      11.745009     9.534576  ...     0.098086     0.453817\n",
              "min       0.000000     0.000000  ...     0.000000     0.000000\n",
              "25%       0.000000     1.000000  ...     0.000000     0.000000\n",
              "50%       3.000000     3.000000  ...     0.000000     0.000000\n",
              "75%       8.000000     7.000000  ...     0.000000     1.000000\n",
              "max     210.000000   132.000000  ...     4.000000     1.000000\n",
              "\n",
              "[8 rows x 3001 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztH-nxpJVEp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fca189e-25f8-42e0-d838-a4cbca6f1a6a"
      },
      "source": [
        "all_data.isnull().sum()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Email No.     0\n",
              "the           0\n",
              "to            0\n",
              "ect           0\n",
              "and           0\n",
              "             ..\n",
              "military      0\n",
              "allowing      0\n",
              "ff            0\n",
              "dry           0\n",
              "Prediction    0\n",
              "Length: 3002, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ol3elgYpVno"
      },
      "source": [
        "sample = all_data.iloc[:,1:]\r\n",
        "\r\n",
        "n_sample = (len(sample))*2\r\n",
        "while len(sample) < n_sample:\r\n",
        "  index = randrange(len(sample))\r\n",
        "  sample_t = sample.iloc[index]\r\n",
        "  sample = sample.append(sample_t, ignore_index=True)\r\n",
        "print(len(sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SybFAHiHYPZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47a4a1e-94ec-45b5-dc89-a1a1de73853b"
      },
      "source": [
        "len_s_1 = int(len(sample)/3)\r\n",
        "len_s_2 = int(2 * len(sample)/3)\r\n",
        "\r\n",
        "X = sample.iloc[:,:-1].values\r\n",
        "Y = sample.iloc[:,-1].values\r\n",
        "# # Split sets to Train & Test sets for part 1\r\n",
        "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2)\r\n",
        "# Make 3 set from dataset for Bagging approach\r\n",
        "X1 = X[:len_s_1,:]\r\n",
        "Y1 = Y[:len_s_1]\r\n",
        "X2 = X[len_s_1:len_s_2, :]\r\n",
        "Y2 = Y[len_s_1:len_s_2]\r\n",
        "X3 = X[len_s_2:, :]\r\n",
        "Y3 = Y[len_s_2:]\r\n",
        "# Split sets to Train & Test sets for part 2\r\n",
        "train_x1, test_x1, train_y1, test_y1 = train_test_split(X1,Y1,test_size = 0.2)\r\n",
        "train_x2, test_x2, train_y2, test_y2 = train_test_split(X2,Y2,test_size = 0.2)\r\n",
        "train_x3, test_x3, train_y3, test_y3 = train_test_split(X3,Y3,test_size = 0.2)\r\n",
        "test_x = np.concatenate((test_x1, test_x2, test_x3), axis=None)\r\n",
        "test_y = np.concatenate((test_y1, test_y2, test_y3), axis=None)\r\n",
        "print(train_x1.shape)\r\n",
        "print(train_y2.shape)\r\n",
        "print(test_y3.shape)\r\n",
        "print(test_y.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2758, 3000)\n",
            "(2758,)\n",
            "(690,)\n",
            "(2070,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQxdRqhwTqhL"
      },
      "source": [
        "Bayesian Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4jK8ZmWzZUz"
      },
      "source": [
        "# Calculate & return probability of set\r\n",
        "def probability(subset):\r\n",
        "\r\n",
        "  sum_s = 0\r\n",
        "  sum_ns = 0\r\n",
        "  for i in subset:\r\n",
        "      if i == 0:\r\n",
        "          sum_s += 1\r\n",
        "      else:\r\n",
        "          sum_ns += 1\r\n",
        "  prob_s = sum_s/len(subset)\r\n",
        "  prob_ns = sum_ns/len(subset)\r\n",
        "  # print(f\"Spam Probability in sample set is {prob_s*100:.2f}%\")\r\n",
        "  # print(f\"Not Spam Probability in sample set is {prob_ns*100:.2f}%\")\r\n",
        "  return prob_s, prob_ns"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxCC6WxUrnNG"
      },
      "source": [
        "P(s|w) = {P(w|s)*P(s)} / {P(w|s)*P(s) + P(w|ns)*P(ns)}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rByXf329Lq_"
      },
      "source": [
        "def bayesian(trainx, trainy, testx, testy):\r\n",
        "\r\n",
        "  # W given S & W given NS\r\n",
        "  ### Calulate number of words in each Spam mail & Ham mail(NotSpam)\r\n",
        "  wi_given_s = [0 for x in trainx[0]]\r\n",
        "  wi_given_ns = [0 for x in trainx[0]]\r\n",
        "  s_count = 0\r\n",
        "  ns_count = 0\r\n",
        "\r\n",
        "  for i,j in zip(trainx, trainy):\r\n",
        "    if j == 0:\r\n",
        "      s_count += sum(i)\r\n",
        "      wi_given_s += i\r\n",
        "    else:\r\n",
        "      ns_count += sum(i)\r\n",
        "      wi_given_ns += i\r\n",
        "\r\n",
        "  # Probability of each word\r\n",
        "  wi_given_s = wi_given_s / s_count\r\n",
        "  # print(max(wi_given_s))\r\n",
        "  wi_given_ns = wi_given_ns / ns_count\r\n",
        "  # print(max(wi_given_ns))\r\n",
        "\r\n",
        "  prob_s, prob_ns = probability(trainy)\r\n",
        "  ### Calculate P(w|s)P(s) + P(w|ns)*P(ns)\r\n",
        "  prob_wi = []\r\n",
        "  for ps, pns in zip(wi_given_s, wi_given_ns):\r\n",
        "    p_wi = (ps * prob_s) + (pns * prob_ns)\r\n",
        "    prob_wi.append(p_wi)\r\n",
        "  # S given W & NS given W\r\n",
        "  s_given_wi = []\r\n",
        "  for si, sj in zip(wi_given_s, prob_wi):\r\n",
        "    ap = (si * prob_s)/sj\r\n",
        "    s_given_wi.append(ap)\r\n",
        "  # print(max(s_given_wi))\r\n",
        "\r\n",
        "  ns_given_wi = []\r\n",
        "  for nsi, nsj in zip(wi_given_ns, prob_wi):\r\n",
        "    apn = (nsi * prob_ns)/nsj\r\n",
        "    ns_given_wi.append(apn)\r\n",
        "  # print(max(ns_given_wi))\r\n",
        "\r\n",
        "  # Classification (Prediction)\r\n",
        "  # P(S|W1)*P(S|W2)*....\r\n",
        "  predict = 0\r\n",
        "  acc_num = 0\r\n",
        "  nb_pred = []\r\n",
        "\r\n",
        "  for xt, yt in zip(testx, testy):\r\n",
        "    sx=0\r\n",
        "    nsx=0\r\n",
        "    # Use LOG for small probability  \r\n",
        "    for xxt, sgx in zip(xt, s_given_wi):\r\n",
        "      if sgx != 0:\r\n",
        "        sx += np.log10(sgx)\r\n",
        "    for xxtn, nsgx in zip(xt, ns_given_wi):\r\n",
        "      if nsgx != 0:\r\n",
        "        nsx += np.log10(nsgx)\r\n",
        "    if sx >= nsx:\r\n",
        "      predict = 1\r\n",
        "    else:\r\n",
        "      predict = 0\r\n",
        "    nb_pred.append(predict)\r\n",
        "    if yt == predict:\r\n",
        "      acc_num += 1\r\n",
        "  accuracy = acc_num / len(testy)\r\n",
        "  print(f\"Bayesian Classification accuracy is {accuracy*100:.2f}%\")\r\n",
        "  return nb_pred\r\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO1gVhi1Mhok"
      },
      "source": [
        "For part 1\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5gHFwvfLBfU",
        "outputId": "6c61f7c7-af38-49a9-a52e-440778821ec6"
      },
      "source": [
        "nb_pred = bayesian(trainX, trainY, testX, testY)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bayesian Classification accuracy is 72.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_76zrrkMxM2"
      },
      "source": [
        "For Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhEpwzLg71QB",
        "outputId": "554cdf4d-31b3-4c90-9d21-0377b9d34e04"
      },
      "source": [
        "nb_pred_1 = bayesian(train_x1, train_y1, test_x1, test_y1)\r\n",
        "nb_pred_2 = bayesian(train_x2, train_y2, test_x2, test_y2)\r\n",
        "nb_pred_3 = bayesian(train_x3, train_y3, test_x3, test_y3)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bayesian Classification accuracy is 70.87%\n",
            "Bayesian Classification accuracy is 70.72%\n",
            "Bayesian Classification accuracy is 70.87%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyclXyjxUNlp"
      },
      "source": [
        "For Suport Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wftjMzHq52B5"
      },
      "source": [
        "class SVM:\r\n",
        "  def __init__(self, learning_rate=0.001, n_iters=250):\r\n",
        "    self.lr = learning_rate\r\n",
        "    self.n_iters = n_iters\r\n",
        "    self.w = None\r\n",
        "    self.b = None\r\n",
        "\r\n",
        "  def fit(self, X, y):\r\n",
        "    n_samples, n_features = X.shape\r\n",
        "    y_ = np.where(y <= 0, -1, 1)   \r\n",
        "    self.w = np.zeros(n_features)\r\n",
        "    self.b = 0\r\n",
        "    for _ in range(self.n_iters):\r\n",
        "      for idx, x_i in enumerate(X):\r\n",
        "        # Compute correct place\r\n",
        "        condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\r\n",
        "        # Update Weight & Bias\r\n",
        "        if condition:\r\n",
        "          self.w -= self.lr * self.w\r\n",
        "        else:\r\n",
        "          self.w -= self.lr * (self.w - np.dot(x_i, y_[idx]))\r\n",
        "          self.b -= self.lr * y_[idx]\r\n",
        "\r\n",
        "  def predict(self, X, y):\r\n",
        "    cnt = 0\r\n",
        "    # Predict result based on Weight & Bias\r\n",
        "    approx = np.dot(X, self.w) - self.b\r\n",
        "    pre = np.sign(approx)\r\n",
        "    for ind, para in enumerate(pre):\r\n",
        "      # On Margin=0 & Spam=1 & NotSpam=-1\r\n",
        "      if para == -1:\r\n",
        "        pre[ind] = 0\r\n",
        "    diff = pre - y\r\n",
        "    for p in diff:\r\n",
        "      if p == 0:\r\n",
        "        cnt += 1\r\n",
        "               \r\n",
        "    print(f\"SVM Classification acuuracy is {(cnt/len(y))*100:.2f}%\")\r\n",
        "    return pre"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVpUks0UuAfg",
        "outputId": "fa2dafd6-56a6-46a7-8971-fd5ddd5d0142"
      },
      "source": [
        "clf = SVM()\r\n",
        "\r\n",
        "clf.fit(train_x1, train_y1)\r\n",
        "svm_pred_1 = clf.predict(test_x1, test_y1)\r\n",
        "clf.fit(train_x2, train_y2)\r\n",
        "svm_pred_2 = clf.predict(test_x2, test_y2)\r\n",
        "clf.fit(train_x3, train_y3)\r\n",
        "svm_pred_3 = clf.predict(test_x3, test_y3)\r\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Classification acuuracy is 80.58%\n",
            "SVM Classification acuuracy is 73.33%\n",
            "SVM Classification acuuracy is 65.36%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qWYNaz29ZED"
      },
      "source": [
        "K_Nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfatWr_XwWJX"
      },
      "source": [
        "def euclidean_distance(x1, x2):\r\n",
        "  return np.sqrt(np.sum((x1 - x2)**2))\r\n",
        "\r\n",
        "class KNN:\r\n",
        "  # KNN has not fitting\r\n",
        "  def fit(self, X, y):\r\n",
        "    self.X_train = X\r\n",
        "    self.y_train = y\r\n",
        "\r\n",
        "  def predict(self, X, Y):\r\n",
        "    y_pred = [self._predict(x) for x in X]\r\n",
        "    diff = list(Y - y_pred)\r\n",
        "    acc = (diff.count(0) / len(Y))*100\r\n",
        "    print(f\"custom KNN classification accuracy is {acc:.2f}%\")\r\n",
        "    # print(y_pred)\r\n",
        "    return y_pred\r\n",
        "\r\n",
        "  def _predict(self, x):\r\n",
        "    # Compute distances between x and all examples in the training set\r\n",
        "    distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\r\n",
        "    # Sort by distance and return indices of the first k neighbors\r\n",
        "    k_idx = np.argsort(distances)[:2]\r\n",
        "    # Extract the labels of the k nearest neighbor training samples\r\n",
        "    k_neighbor_labels = [self.y_train[i] for i in k_idx]  \r\n",
        "    # return the most common class label\r\n",
        "    most_common = Counter(k_neighbor_labels).most_common(1)\r\n",
        "    return most_common[0][0]\r\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G63U8DEW9qa6",
        "outputId": "bcb77d73-ca18-4663-e09a-fd20ee32f072"
      },
      "source": [
        "clf = KNN()\r\n",
        "\r\n",
        "clf.fit(train_x1, train_y1)\r\n",
        "knn_pred_1 = clf.predict(test_x1, test_y1)\r\n",
        "clf.fit(train_x2, train_y2)\r\n",
        "knn_pred_2 = clf.predict(test_x2, test_y2)\r\n",
        "clf.fit(train_x3, train_y3)\r\n",
        "knn_pred_3 = clf.predict(test_x3, test_y3)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "custom KNN classification accuracy is 85.22%\n",
            "custom KNN classification accuracy is 89.13%\n",
            "custom KNN classification accuracy is 93.19%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68lowErSiO_R"
      },
      "source": [
        "Ensemble Learning with Max-Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaVtOtHw31UU",
        "outputId": "ccf47858-31c8-45ab-f9d8-799a24df42dc"
      },
      "source": [
        "### Ensemble Learning (Bagging)\r\n",
        "max_vote_pred = []\r\n",
        "counter = 0\r\n",
        "final_pred_1 = [nb_pred_1, svm_pred_1, knn_pred_1]\r\n",
        "final_pred_2 = [nb_pred_2, svm_pred_2, knn_pred_2]\r\n",
        "final_pred_3 = [nb_pred_3, svm_pred_3, knn_pred_3]\r\n",
        "\r\n",
        "# Find max class for each email\r\n",
        "def max_vote(final_pred):\r\n",
        "  sum_pred = []\r\n",
        "  sum_pred = [sum(i) for i in zip(*final_pred)]\r\n",
        "  for p in sum_pred:\r\n",
        "    if p >= 2:\r\n",
        "      max_vote_pred.append(1)\r\n",
        "    else:\r\n",
        "      max_vote_pred.append(0)\r\n",
        "\r\n",
        "max_vote(final_pred_1)\r\n",
        "max_vote(final_pred_2)\r\n",
        "max_vote(final_pred_3)\r\n",
        "# Accuracy\r\n",
        "f_diff = test_y - max_vote_pred\r\n",
        "for fd in f_diff:\r\n",
        "  if fd == 0:\r\n",
        "    counter += 1\r\n",
        "# print(counter)\r\n",
        "final_acc = counter / (len(test_y))\r\n",
        "print(f\"Ensemble learning's accuracy is {final_acc*100:.2f}%\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ensemble learning's accuracy is 82.85%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}